{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 12 - Processamento de Linguagem Natural</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.7.6\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: Este é um material de bônus incluído neste curso. PyTorch é estudado em detalhes no curso <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-frameworks\">Deep Learning Frameworks</a> e aplicado em PLN no curso <a href=\"https://www.datascienceacademy.com.br/course?courseid=processamento-de-linguagem-natural-e-reconhecimento-de-voz\">Processamento de Linguagem Natural</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudo de Caso - Tradutor de Idioma com Machine Learning e PLN\n",
    "\n",
    "![title](imagens/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A definição deste estudo de caso está no manual em pdf no Capítulo 12 do Curso de <a href=\"https://www.datascienceacademy.com.br/course?courseid=machine-learning-engineer\">Machine Learning</a>**. \n",
    "\n",
    "Faça a leitura do manual antes de prosseguir com o Estudo de Caso.\n",
    "\n",
    "O dataset que usaremos oferece texto somente em inglês, alemão e francês. Como nosso objetivo é estudar a construção do modelo e não os idiomas, isso não faz diferença e trabalharemos com tradução inglês/alemão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install torch==1.5.0\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o PyTorch\n",
    "!pip install -q torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O pacote torchtext fornece diversos datasets e funções para PLN\n",
    "# https://torchtext.readthedocs.io/en/latest/index.html\n",
    "!pip install -q torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o spacy\n",
    "!pip install -q spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy     2.2.4\n",
      "numpy     1.18.4\n",
      "torch     1.5.0\n",
      "torchtext 0.6.0\n",
      "Data Science Academy\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: O treinamento do modelo deste estudo de caso é computacionalmente intensivo e por isso treinamos o modelo no Titan, o super servidor da DSA, com 3 GPUs e 128 GB de Memória RAM. O acesso a esse servidor é gratuito para alunos das Formações:\n",
    "\n",
    "- <a href=\"https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\">Formação Inteligência Artificial</a>\n",
    "- <a href=\"https://www.datascienceacademy.com.br/pages/formacao-ia-aplicada-a-medicina\">Formação Inteligência Artificial Aplicada à Medicina</a>\n",
    "- <a href=\"https://www.datascienceacademy.com.br/pages/formacao-engenheiro-blockchain\">Formação Engenheiro Blockchain</a>\n",
    "\n",
    "O treinamento pode ser feito em um computador apenas com CPU. O tempo de treinamento será um pouco maior, mas o estudo de caso poderá ser executado sem problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui definimos o device que será usado para treinar o modelo\n",
    "# Se pelo menos uma GPU estiver disponível, usaremos o device 'cuda' (nome da plataforma da Nvidia para GPU)\n",
    "# Se não tiver GPU, usaremos a CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo a descrição das GPUs do servidor da DSA. O comando abaixo funcionará somente se a plataforma CUDA da Nivida estiver instalada no computador. Se quiser conhecer mais sobre a plataforma CUDA, acesse aqui:\n",
    "\n",
    "https://developer.nvidia.com/cuda-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 21 21:49:41 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN X (Pascal)    On   | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 23%   41C    P8     9W / 250W |    125MiB / 12194MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  On   | 00000000:09:00.0 Off |                  N/A |\r\n",
      "| 23%   38C    P8     9W / 250W |     12MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN RTX           On   | 00000000:0B:00.0 Off |                  N/A |\r\n",
      "| 41%   39C    P8    13W / 280W |     12MiB / 24220MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1578      G   /usr/lib/xorg/Xorg                            39MiB |\r\n",
      "|    0      1614      G   /usr/bin/gnome-shell                          72MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# GPUs no servidor da DSA\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dicionários\n",
    "\n",
    "Precisamos instalar os dicionários dos idiomas que serão usados para treinar o modelo. Aqui você encontra detalhes sobre os datasets:\n",
    "\n",
    "https://www.statmt.org/wmt16/multimodal-task.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /home/dmpm/anaconda3/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/dmpm/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/dmpm/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/dmpm/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# Download do dicionário em inglês\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /home/dmpm/anaconda3/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/dmpm/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dmpm/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/dmpm/anaconda3/lib/python3.7/site-packages/de_core_news_sm -->\n",
      "/home/dmpm/anaconda3/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "# Download do dicionário em alemão\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora carregamos os dicionários na memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dicionários\n",
    "spacy_german = spacy.load('de')\n",
    "spacy_english = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar duas funções para tokenização dos dicionários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tokenização do dicionário em inglês\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tokenização do dicionário em alemão\n",
    "def tokenize_german(text):\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos agora criar a fonte e o destino, ou seja, o idioma fonte e o idioma destino para nosso tradutor.\n",
    "\n",
    "Nosso modelo deverá fazer a tradução do inglês para o alemão. Inglês será a fonte (SOURCE) e Alemão será o destino (TARGET)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idioma de origem\n",
    "SOURCE = Field(tokenize = tokenize_english, init_token = '<sos>', eos_token = '<eos>', lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idioma de destino\n",
    "TARGET = Field(tokenize = tokenize_german, init_token = '<sos>', eos_token = '<eos>', lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos a split() do pacote Multi30k do torchtext para separar os dicionários em SOURCE e TARGET \n",
    "# e então em treino, validação e teste\n",
    "# Obs: Será feito o download dos dados no pacote Multi30k\n",
    "dados_treino, dados_valid, dados_teste = Multi30k.splits(exts = ('.en', '.de'), fields = (SOURCE, TARGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'bushes', 'many', 'near', 'outside', 'are', 'males', 'white', ',', 'young', 'two']\n",
      "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n"
     ]
    }
   ],
   "source": [
    "# Visualizando os dados de treino, SOURCE e TARGET\n",
    "print(dados_treino.examples[0].src)\n",
    "print(dados_treino.examples[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Dataset de Treino: 29000\n",
      "Tamanho do Dataset de Validação: 1014\n",
      "Tamanho do Dataset de Teste: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do Dataset de Treino: \" + str(len(dados_treino.examples)))\n",
    "print(\"Tamanho do Dataset de Validação: \" + str(len(dados_valid.examples)))\n",
    "print(\"Tamanho do Dataset de Teste: \" + str(len(dados_teste.examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar os vocabulários de SOURCE  e TARGET\n",
    "SOURCE.build_vocab(dados_treino, min_freq = 2)\n",
    "TARGET.build_vocab(dados_treino, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Vocabulário em Inglês (SOURCE): 5893\n",
      "Tamanho do Vocabulário em Alemão (TARGET): 7855\n"
     ]
    }
   ],
   "source": [
    "# Print do tamanho dos vocabulários\n",
    "print(\"Tamanho do Vocabulário em Inglês (SOURCE): \" + str(len(SOURCE.vocab)))\n",
    "print(\"Tamanho do Vocabulário em Alemão (TARGET): \" + str(len(TARGET.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o Modelo\n",
    "\n",
    "Criaremos 3 classes:\n",
    "\n",
    "- Encoder\n",
    "- Decoder\n",
    "- Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para o Encoder\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    # Método construtor\n",
    "    def __init__(self, input_dims, emb_dims, hid_dims, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Camadas do modelo\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dims, emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Método forward para o treinamento\n",
    "    def forward(self, src):\n",
    "        \n",
    "        # Execução do modelo\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (h, cell) = self.rnn(embedded)\n",
    "        \n",
    "        return h, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para o Decoder\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    # Método construtor\n",
    "    def __init__(self, output_dims, emb_dims, hid_dims, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Camadas do modelo\n",
    "        self.output_dims = output_dims\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dims, emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dims, output_dims)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Método forward para o treinamento\n",
    "    def forward(self, input, h, cell):\n",
    "              \n",
    "        # Execução do modelo\n",
    "        input = input.unsqueeze(0)   \n",
    "        embedded = self.dropout(self.embedding(input))     \n",
    "        output, (h, cell) = self.rnn(embedded, (h, cell))\n",
    "        pred = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return pred, h, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para o modelo Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    # Método construtor\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Componentes do modelo\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    # Método forward para o treinamento\n",
    "    def forward(self, src, trg, teacher_forcing_rate = 0.5):\n",
    "        \n",
    "        # Execução do modelo\n",
    "        batch_size = trg.shape[1]\n",
    "        target_length = trg.shape[0]\n",
    "        target_vocab_size = self.decoder.output_dims\n",
    "        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)\n",
    "        h, cell = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "\n",
    "            output, h, cell = self.decoder(input, h, cell)\n",
    "            outputs[t] = output\n",
    "            top = output.argmax(1) \n",
    "            input = trg[t] if (random.random() < teacher_forcing_rate) else top\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir alguns hiperparâmetros e os gerados de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "batch_size = 32\n",
    "input_dimensions = len(SOURCE.vocab)\n",
    "output_dimensions = len(TARGET.vocab)\n",
    "encoder_embedding_dimensions = 256\n",
    "decoder_embedding_dimensions = 256\n",
    "hidden_layer_dimensions = 512\n",
    "num_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "epochs = 20\n",
    "grad_clip = 1\n",
    "lowest_validation_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geradores de dados\n",
    "iterador_treino, iterador_valid, iterador_teste = BucketIterator.splits((dados_treino, dados_valid, dados_teste), \n",
    "                                                                        batch_size = batch_size, \n",
    "                                                                        device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui nós criamos o encoder, decoder e o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instância do Encoder\n",
    "encod = Encoder(input_dimensions, \n",
    "                encoder_embedding_dimensions,\n",
    "                hidden_layer_dimensions, \n",
    "                num_layers, \n",
    "                encoder_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instância do Decoder\n",
    "decod = Decoder(output_dimensions, \n",
    "                decoder_embedding_dimensions,\n",
    "                hidden_layer_dimensions, \n",
    "                num_layers, \n",
    "                decoder_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instância do Modelo\n",
    "modelo = Seq2Seq(encod, decod, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=7855, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo criado\n",
    "modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir a função de inicalização dos pesos, função de custo e otimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisamos de uma função para inicializar os pesos da rede neural\n",
    "def inicializa_pesos(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=7855, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Incluímos a função de inicialização dos pesos no modelo\n",
    "modelo.apply(inicializa_pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos a função de custo para calcular o erro do modelo\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TARGET.vocab.stoi[TARGET.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos o otimizador para atualizar os pesos do modelo a cada passada de treinamento\n",
    "optimizer = optim.Adam(modelo.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora não seja obrigatório, criar funções para treino e avaliação do modelo ajuda a modularizar nosso processo de treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar o modelo\n",
    "def treina_modelo(modelo, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    # Inicia o método de treinamento\n",
    "    modelo.train()\n",
    "    \n",
    "    # Inicializa o erro da epoch\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Loop pelo iterador (gerador de dados)\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        # Coletamos dados fonte e destino\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        # Zeramos os gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Fazemos as previsões com o modelo\n",
    "        output = modelo(src, trg)\n",
    "        \n",
    "        # Ajustamos o shape das previsões\n",
    "        output_dims = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dims)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # Calculamos o erro do modelo\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Iniciamos o backpropgation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calculamos os gradientes da derivada para atualização dos pesos\n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), clip)\n",
    "        \n",
    "        # Aplicamos a atualização dos pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Armazenamos o erro da epoch\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para avaliar o modelo\n",
    "def avalia_modelo(modelo, iterator, criterion):\n",
    "    \n",
    "    # Inicia função de avaiação\n",
    "    modelo.eval()\n",
    "    \n",
    "    # Inicializa o erro da epoch\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Vamos fazer as previsões com o modelo\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Loop pelo iterador (gerador de dados)\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            # Extrai fonte e destino\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # Previsão com o modelo\n",
    "            output = modelo(src, trg, 0)\n",
    "\n",
    "            # Ajusta as dimensões das previsões\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Calcula o erro do modelo\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            # Armazena o erro na epoch\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o Modelo\n",
    "\n",
    "O treinamento do modelo é demorado. Seja paciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 48.0s\n",
      "\tErro em Treino: 4.7076\n",
      "\t Erro em Validação: 4.6512\n",
      "Epoch: 02 | Time: 47.0s\n",
      "\tErro em Treino: 3.9973\n",
      "\t Erro em Validação: 4.3468\n",
      "Epoch: 03 | Time: 47.0s\n",
      "\tErro em Treino: 3.6368\n",
      "\t Erro em Validação: 4.0514\n",
      "Epoch: 04 | Time: 48.0s\n",
      "\tErro em Treino: 3.3807\n",
      "\t Erro em Validação: 3.8883\n",
      "Epoch: 05 | Time: 48.0s\n",
      "\tErro em Treino: 3.1684\n",
      "\t Erro em Validação: 3.8140\n",
      "Epoch: 06 | Time: 48.0s\n",
      "\tErro em Treino: 2.9915\n",
      "\t Erro em Validação: 3.6792\n",
      "Epoch: 07 | Time: 47.0s\n",
      "\tErro em Treino: 2.8233\n",
      "\t Erro em Validação: 3.6726\n",
      "Epoch: 08 | Time: 48.0s\n",
      "\tErro em Treino: 2.6837\n",
      "\t Erro em Validação: 3.6291\n",
      "Epoch: 09 | Time: 48.0s\n",
      "\tErro em Treino: 2.5523\n",
      "\t Erro em Validação: 3.6190\n",
      "Epoch: 10 | Time: 47.0s\n",
      "\tErro em Treino: 2.4393\n",
      "\t Erro em Validação: 3.5998\n",
      "Epoch: 11 | Time: 47.0s\n",
      "\tErro em Treino: 2.3307\n",
      "\t Erro em Validação: 3.5683\n",
      "Epoch: 12 | Time: 48.0s\n",
      "\tErro em Treino: 2.2307\n",
      "\t Erro em Validação: 3.5983\n",
      "Epoch: 13 | Time: 48.0s\n",
      "\tErro em Treino: 2.1253\n",
      "\t Erro em Validação: 3.5880\n",
      "Epoch: 14 | Time: 47.0s\n",
      "\tErro em Treino: 2.0154\n",
      "\t Erro em Validação: 3.6639\n",
      "Epoch: 15 | Time: 48.0s\n",
      "\tErro em Treino: 1.9392\n",
      "\t Erro em Validação: 3.6356\n",
      "Epoch: 16 | Time: 48.0s\n",
      "\tErro em Treino: 1.8669\n",
      "\t Erro em Validação: 3.6467\n",
      "Epoch: 17 | Time: 48.0s\n",
      "\tErro em Treino: 1.7866\n",
      "\t Erro em Validação: 3.7214\n",
      "Epoch: 18 | Time: 48.0s\n",
      "\tErro em Treino: 1.7091\n",
      "\t Erro em Validação: 3.7471\n",
      "Epoch: 19 | Time: 47.0s\n",
      "\tErro em Treino: 1.6635\n",
      "\t Erro em Validação: 3.7931\n",
      "Epoch: 20 | Time: 48.0s\n",
      "\tErro em Treino: 1.6003\n",
      "\t Erro em Validação: 3.8183\n"
     ]
    }
   ],
   "source": [
    "# Loop pelo número de epochs para treinar o modelo\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Grava o tempo quando começamos\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Treinamento\n",
    "    train_loss = treina_modelo(modelo, iterador_treino, optimizer, criterion, grad_clip)\n",
    "    \n",
    "    # Validação\n",
    "    valid_loss = avalia_modelo(modelo, iterador_valid, criterion)\n",
    "    \n",
    "    # Grava o tempo quando finalizamos\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Verificamos o erro mínimo e então salvamos o modelo fazendo um checkpoint do modelo com melhor performance\n",
    "    if valid_loss < lowest_validation_loss:\n",
    "        lowest_validation_loss = valid_loss\n",
    "        torch.save(modelo.state_dict(), 'modelos/seq2seq.pt')\n",
    "    \n",
    "    # Print\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {np.round(end_time-start_time,0)}s')\n",
    "    print(f'\\tErro em Treino: {train_loss:.4f}')\n",
    "    print(f'\\t Erro em Validação: {valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando o Modelo\n",
    "\n",
    "Com o modelo treinado, avaliamos com dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregamos o modelo treinado\n",
    "modelo.load_state_dict(torch.load('modelos/seq2seq.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliamos o modelo\n",
    "test_loss = avalia_modelo(modelo, iterador_teste, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro em Teste: 3.5024\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "print(f'Erro em Teste: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traduzindo Idioma\n",
    "\n",
    "Modelo treinado e avaliado, vamos usá-lo para o fim para o qual ele foi criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tradução de idioma em 5 sentenças\n",
    "def traduz_idioma(modelo, iterator, limit = 5):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Loop pelo iterador\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            # Enquanto estivermos dentro do limite, vamos fazendo tradução\n",
    "            if i < limit :\n",
    "                \n",
    "                # Extraímos SOURCE e TARGET\n",
    "                # Fazemos isso para poder comparar a tradução correta com a previsão\n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "\n",
    "                # Previsão do modelo\n",
    "                output = modelo(src, trg, 0)\n",
    "                \n",
    "                # Todas as previsões\n",
    "                preds = torch.tensor([[torch.argmax(x).item()] for x in output])\n",
    "                \n",
    "                # Prints\n",
    "                print('Texto de Origem em Inglês: ' + str([SOURCE.vocab.itos[x] for x in src][1:-1][::-1]))\n",
    "                print('Texto de Destino em Alemão (Valor Esperado): ' + str([TARGET.vocab.itos[x] for x in trg][1:-1]))\n",
    "                print('Texto de Destino em Alemão (Valor Previsto): ' + str([TARGET.vocab.itos[x] for x in preds][1:-1]))\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos gerar texto randômico a partir dos dados disponíveis\n",
    "_, _, iterador_translate = BucketIterator.splits((dados_treino, dados_valid, dados_teste), \n",
    "                                                 batch_size = 1, \n",
    "                                                 device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de Origem em Inglês: ['two', 'men', 'wearing', 'hats', '.']\n",
      "Texto de Destino em Alemão (Valor Esperado): ['zwei', 'männer', 'mit', 'mützen', '.']\n",
      "Texto de Destino em Alemão (Valor Previsto): ['zwei', 'männer', 'mit', 'schwarzen', 'haaren']\n",
      "\n",
      "\n",
      "Texto de Origem em Inglês: ['young', 'woman', 'climbing', 'rock', 'face']\n",
      "Texto de Destino em Alemão (Valor Esperado): ['junge', 'frau', 'klettert', 'auf', 'felswand']\n",
      "Texto de Destino em Alemão (Valor Previsto): ['eine', 'junge', 'frau', 'klettert', 'eine']\n",
      "\n",
      "\n",
      "Texto de Origem em Inglês: ['a', 'woman', 'is', 'playing', 'volleyball', '.']\n",
      "Texto de Destino em Alemão (Valor Esperado): ['eine', 'frau', 'spielt', 'volleyball', '.']\n",
      "Texto de Destino em Alemão (Valor Previsto): ['eine', 'frau', 'spielt', 'volleyball', '.']\n",
      "\n",
      "\n",
      "Texto de Origem em Inglês: ['three', 'men', 'are', 'walking', 'up', 'hill', '.']\n",
      "Texto de Destino em Alemão (Valor Esperado): ['drei', 'männer', 'gehen', 'bergauf', '.']\n",
      "Texto de Destino em Alemão (Valor Previsto): ['drei', 'männer', 'gehen', 'durch', 'einen']\n",
      "\n",
      "\n",
      "Texto de Origem em Inglês: ['an', 'army', 'officer', 'is', 'inspecting', 'something', '.']\n",
      "Texto de Destino em Alemão (Valor Esperado): ['ein', '<unk>', 'inspiziert', 'etwas', '.']\n",
      "Texto de Destino em Alemão (Valor Previsto): ['ein', '<unk>', 'schaut', 'in', 'die']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tradução de idioma\n",
    "saida = traduz_idioma(modelo, iterador_translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns! Aí está está seu tradutor de texto com Machine Learning e PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
